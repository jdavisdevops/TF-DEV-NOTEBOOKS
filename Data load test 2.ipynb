{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latin-replica",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy at 0x23fd3df2b20>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nvitop\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "# os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "registered-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPU will likely run quickly with dtype policy mixed_float16 as it has compute capability of at least 7.0. Your GPU: NVIDIA GeForce GTX 1660, compute capability 7.5\n"
     ]
    }
   ],
   "source": [
    "policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "tf.keras.mixed_precision.experimental.set_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "located-arrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = pathlib.Path(\n",
    "    r\"C:\\Users\\josephdavis\\Desktop\\Tensorflow notebooks\\cats-v-dogs\\training\"\n",
    ")\n",
    "valdir = pathlib.Path(\n",
    "    r\"C:\\Users\\josephdavis\\Desktop\\Tensorflow notebooks\\cats-v-dogs\\testing\"\n",
    ")\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "logdir = pathlib.Path.cwd() / \"logs\"\n",
    "\n",
    "if len(os.listdir(logdir)) > 5:\n",
    "    shutil.rmtree(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "parliamentary-butler",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "# from PIL import Image\n",
    "# def check_images( s_dir, ext_list):\n",
    "#     bad_images=[]\n",
    "#     bad_ext=[]\n",
    "#     s_list= os.listdir(s_dir)\n",
    "#     for klass in s_list:\n",
    "#         klass_path=os.path.join (s_dir, klass)\n",
    "#         print ('processing class directory ', klass)\n",
    "#         if os.path.isdir(klass_path):\n",
    "#             file_list=os.listdir(klass_path)\n",
    "#             for f in file_list:\n",
    "#                 f_path=os.path.join (klass_path,f)\n",
    "#                 index=f.rfind('.')\n",
    "#                 ext=f[index+1:].lower()\n",
    "#                 if ext not in ext_list:\n",
    "#                     print('file ', f_path, ' has an invalid extension ', ext)\n",
    "#                     bad_ext.append(f_path)\n",
    "#                     os.remove(f_path)\n",
    "#                     print(\"removed bad extension file\")\n",
    "#                 if os.path.isfile(f_path):\n",
    "#                     try:\n",
    "#                         img=cv2.imread(f_path)\n",
    "#                         shape=img.shape\n",
    "#                     except:\n",
    "#                         print('file ', f_path, ' is not a valid image file')\n",
    "#                         bad_images.append(f_path)\n",
    "# #                         img = Image.open(f_path)\n",
    "# #                         img = img.save(f_path + \".jpeg\")\n",
    "#                         os.remove(f_path)\n",
    "#                         print('Removed bad image')\n",
    "#                 else:\n",
    "#                     print('*** fatal error, you a sub directory ', f, ' in class directory ', klass)\n",
    "#         else:\n",
    "#             print ('*** WARNING*** you have files in ', s_dir, ' it should only contain sub directories')\n",
    "#     return bad_images, bad_ext\n",
    "# ext_list = [\"jpg\"]\n",
    "# check_images(datadir, ext_list)\n",
    "# check_images(valdir, ext_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "successful-range",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22283\n"
     ]
    }
   ],
   "source": [
    "image_count = len(list(datadir.glob(\"*/*.jpg\")))\n",
    "print(image_count)\n",
    "batch_size = 128\n",
    "img_height = 150\n",
    "img_width = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "operational-constitutional",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(str(datadir / \"*/*.jpg\"), shuffle=False)\n",
    "val_ds = tf.data.Dataset.list_files(str(valdir / \"*/*.jpg\"), shuffle=False)\n",
    "list_ds = list_ds.shuffle(image_count, reshuffle_each_iteration=False)\n",
    "num_train_files = len(list_ds)\n",
    "num_val_files = len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "alone-arrest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\training\\\\cats\\\\7973.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\training\\\\dogs\\\\10003.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\training\\\\cats\\\\11674.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\training\\\\dogs\\\\7023.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\training\\\\cats\\\\11055.jpg'\n",
      "validation\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\testing\\\\cats\\\\100.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\testing\\\\cats\\\\10004.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\testing\\\\cats\\\\10024.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\testing\\\\cats\\\\10038.jpg'\n",
      "b'C:\\\\Users\\\\josephdavis\\\\Desktop\\\\Tensorflow notebooks\\\\cats-v-dogs\\\\testing\\\\cats\\\\10052.jpg'\n"
     ]
    }
   ],
   "source": [
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())\n",
    "print(\"validation\")\n",
    "for f in val_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "executive-buyer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cats' 'dogs']\n"
     ]
    }
   ],
   "source": [
    "class_names = np.array(sorted([item.name for item in datadir.glob(\"*\")]))\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "subject-plasma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_size = int(image_count * .2)\n",
    "# train_ds = list_ds.skip(val_size)\n",
    "# val_ds = list_ds.take(val_size)\n",
    "# train_ds = list_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pregnant-subdivision",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22283\n",
      "2473\n"
     ]
    }
   ],
   "source": [
    "print(tf.data.experimental.cardinality(list_ds).numpy())\n",
    "print(tf.data.experimental.cardinality(val_ds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "handled-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(file_path):\n",
    "    parts = tf.strings.split(file_path, os.path.sep)\n",
    "    one_hot = parts[-2] == class_names\n",
    "    return tf.argmax(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "convertible-sucking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    #     img = img/255.\n",
    "    return img  # tf.image.resize(img, [img_height, img_width])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gothic-residence",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    #     return tf.data.Dataset.from_tensors((img, label))\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fiscal-shipping",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = list_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "val_ds = val_ds.map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# train_ds = list_ds.interleave(\n",
    "#     lambda x: tf.data.Dataset.list_files(str(datadir / \"*/*.jpg\"), shuffle=True),\n",
    "#     num_parallel_calls=AUTOTUNE,\n",
    "#     cycle_length=4,\n",
    "# #     block_length=4,\n",
    "# #     deterministic=False,\n",
    "# ).map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "# val_ds = val_ds.interleave(\n",
    "#     lambda x: tf.data.Dataset.list_files(str(valdir / \"*/*.jpg\"), shuffle=True),\n",
    "#     num_parallel_calls=AUTOTUNE,\n",
    "#     cycle_length=4,\n",
    "# #     block_length=4,\n",
    "# #     deterministic=False,\n",
    "# ).map(process_path, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "# train_ds.cache()\n",
    "# val_ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "unsigned-voice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for image, label in train_ds.take(1):\n",
    "#     print(\"Image Shape: \", image.numpy().shape)\n",
    "#     print(\"Label: \", label.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "medical-variation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential(\n",
    "    [\n",
    "        #         layers.experimental.preprocessing.RandomFlip(\"horizontal_and_vertical\"),\n",
    "        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "        #         layers.experimental.preprocessing.RandomTranslation(0.2, 0.2),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.2, interpolation=\"bilinear\"),\n",
    "        layers.experimental.preprocessing.RandomWidth(0.01, interpolation=\"bilinear\"),\n",
    "        layers.experimental.preprocessing.RandomHeight(0.01, interpolation=\"bilinear\"),\n",
    "        #         layers.experimental.preprocessing.RandomContrast(0.2),\n",
    "        #         layers.experimental.preprocessing.RandomZoom(0.1),\n",
    "    ]\n",
    ")\n",
    "\n",
    "resize_and_rescale = tf.keras.Sequential(\n",
    "    [\n",
    "        layers.experimental.preprocessing.Resizing(img_height, img_width),\n",
    "        layers.experimental.preprocessing.Rescaling(1.0 / 255),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "processed-rebound",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare(ds, shuffle=False, augment=False):\n",
    "\n",
    "    # Resize and rescale all datasets.\n",
    "    ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "    ds.cache()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1000)\n",
    "\n",
    "    # Batch all datasets\n",
    "    ds = ds.batch(batch_size)\n",
    "\n",
    "    if augment:\n",
    "        ds = ds.map(\n",
    "            lambda x, y: (data_augmentation(x, training=True), y),\n",
    "            num_parallel_calls=AUTOTUNE,\n",
    "        ).cache()\n",
    "\n",
    "    # Use buffered prefecting on all datasets\n",
    "    return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "val_ds = prepare(val_ds)\n",
    "\n",
    "# options = tf.data.Options()\n",
    "# options.experimental_threading.max_intra_op_parallelism = 1\n",
    "# train_ds = train_ds.with_options(options)\n",
    "# val_ds = val_ds.with_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "twelve-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prepare(ds, shuffle=False, augment=False):\n",
    "#     # Resize and rescale all datasets\n",
    "#     ds = ds.map(lambda x, y: (resize_and_rescale(x), y), num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "#     if shuffle:\n",
    "#         ds = ds.shuffle(1000)\n",
    "\n",
    "#     # Batch all datasets\n",
    "#     ds = ds.batch(batch_size)\n",
    "\n",
    "#     # Use data augmentation only on the training set\n",
    "#     if augment:\n",
    "#         ds = ds.map(\n",
    "#             lambda x, y: (data_augmentation(x, training=True), y),\n",
    "#             num_parallel_calls=AUTOTUNE,\n",
    "#         )\n",
    "#     ds.cache()\n",
    "#     # Use buffered prefecting on all datasets\n",
    "#     return ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "\n",
    "# train_ds = prepare(train_ds, shuffle=True, augment=True)\n",
    "# val_ds = prepare(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "streaming-record",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def configure_for_performance(ds):\n",
    "#     ds = ds.cache()\n",
    "#     ds = ds.shuffle(buffer_size=1000)\n",
    "#     ds = ds.batch(batch_size)\n",
    "#     ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "#     return ds\n",
    "# train_ds = configure_for_performance(train_ds)\n",
    "# val_ds = configure_for_performance(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "laughing-protest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# img_batch, label_batch = next(iter(train_ds))\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(img_batch.numpy()[i])\n",
    "#     label = label_batch[i]\n",
    "#     plt.title(class_names[label])\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "pharmaceutical-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last layer output shape:  (None, 7, 7, 768)\n"
     ]
    }
   ],
   "source": [
    "pre_trained_model = InceptionV3(\n",
    "    input_shape=(img_height, img_width, 3), include_top=False, weights=\"imagenet\"\n",
    ")\n",
    "\n",
    "# weights_file = pathlib.Path.cwd() / 'inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
    "# pre_trained_model.load_weights(weights_file)\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "last_layer = pre_trained_model.get_layer(\"mixed7\")\n",
    "print(\"last layer output shape: \", last_layer.output_shape)\n",
    "last_output = last_layer.output\n",
    "\n",
    "x = layers.Flatten()(last_output)\n",
    "x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = Model(pre_trained_model.input, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=RMSprop(lr=0.0001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "transparent-march",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.models.Sequential(\n",
    "#     [\n",
    "#         # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
    "#         tf.keras.layers.Conv2D(\n",
    "#             16, (3, 3), activation=\"relu\", input_shape=(150, 150, 3)\n",
    "#         ),\n",
    "#         tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#         tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "#         tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#         tf.keras.layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "#         tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#         tf.keras.layers.Conv2D(128, (3, 3), activation=\"relu\"),\n",
    "#         tf.keras.layers.MaxPooling2D(2, 2),\n",
    "# #         tf.keras.layers.Conv2D(512, (3, 3), activation=\"relu\"),\n",
    "# #         tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#         # Flatten the results to feed into a DNN\n",
    "#         tf.keras.layers.Flatten(),\n",
    "#         # 512 neuron hidden layer\n",
    "#         tf.keras.layers.Dense(512, activation=\"relu\"),\n",
    "#         tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "#         # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
    "#         tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=RMSprop(lr=0.001), loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n",
    "# )\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "transsexual-wireless",
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = \"logs/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "tboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logs, histogram_freq=1, profile_batch=\"500,520\"\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_accuracy\", mode=\"auto\", patience=4, restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cleared-rubber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "175/175 [==============================] - 71s 360ms/step - loss: 0.4139 - accuracy: 0.8478 - val_loss: 0.1046 - val_accuracy: 0.9660\n",
      "Epoch 2/50\n",
      "175/175 [==============================] - 57s 327ms/step - loss: 0.1346 - accuracy: 0.9454 - val_loss: 0.1114 - val_accuracy: 0.9664\n",
      "Epoch 3/50\n",
      "175/175 [==============================] - 59s 336ms/step - loss: 0.0712 - accuracy: 0.9736 - val_loss: 0.1165 - val_accuracy: 0.9685\n",
      "Epoch 4/50\n",
      "175/175 [==============================] - 57s 327ms/step - loss: 0.0480 - accuracy: 0.9843 - val_loss: 0.1241 - val_accuracy: 0.9697\n",
      "Epoch 5/50\n",
      "175/175 [==============================] - 57s 328ms/step - loss: 0.0302 - accuracy: 0.9891 - val_loss: 0.1614 - val_accuracy: 0.9672\n",
      "Epoch 6/50\n",
      "175/175 [==============================] - 57s 328ms/step - loss: 0.0292 - accuracy: 0.9923 - val_loss: 0.1543 - val_accuracy: 0.9656\n",
      "Epoch 7/50\n",
      "175/175 [==============================] - 57s 328ms/step - loss: 0.0164 - accuracy: 0.9948 - val_loss: 0.1706 - val_accuracy: 0.9677\n",
      "Epoch 8/50\n",
      "175/175 [==============================] - 57s 328ms/step - loss: 0.0178 - accuracy: 0.9953 - val_loss: 0.1878 - val_accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=50,\n",
    "    #     steps_per_epoch=100,\n",
    "    #     validation_steps=50,\n",
    "    verbose=1,\n",
    "    callbacks=[callback, tboard_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "victorian-argentina",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 58252), started 7 days, 6:18:09 ago. (Use '!kill 58252' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7b612af5eaa13520\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7b612af5eaa13520\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "outer-absence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n",
      "127.jpg is a dog\n",
      "[0.5]\n",
      "137.jpg is a dog\n",
      "[0.5]\n",
      "140.jpg is a dog\n",
      "[0.5]\n",
      "159.jpg is a dog\n",
      "[0.5]\n",
      "196.jpg is a dog\n",
      "[0.5]\n",
      "205.jpg is a dog\n",
      "[0.5]\n",
      "218.jpg is a dog\n",
      "[0.5]\n",
      "220.jpg is a dog\n",
      "[0.5]\n",
      "222.jpg is a dog\n",
      "[0.5]\n",
      "229.jpg is a dog\n",
      "[0.5]\n",
      "232.jpg is a dog\n",
      "[0.5]\n",
      "243.jpg is a dog\n",
      "[0.5]\n",
      "274.jpg is a dog\n",
      "[0.5]\n",
      "279.jpg is a dog\n",
      "[0.5]\n",
      "285.jpg is a dog\n",
      "[0.5]\n",
      "321.jpg is a dog\n",
      "[0.5]\n",
      "326.jpg is a dog\n",
      "[0.5]\n",
      "330.jpg is a dog\n",
      "[0.5]\n",
      "338.jpg is a dog\n",
      "[0.5]\n",
      "345.jpg is a dog\n",
      "[0.5]\n",
      "346.jpg is a dog\n",
      "[0.5]\n",
      "353.jpg is a dog\n",
      "[0.5]\n",
      "363.jpg is a dog\n",
      "[0.5]\n",
      "365.jpg is a dog\n",
      "[0.5]\n",
      "402.jpg is a dog\n",
      "[0.5]\n",
      "407.jpg is a dog\n",
      "[0.5]\n",
      "409.jpg is a dog\n",
      "[0.5]\n",
      "425.jpg is a dog\n",
      "[0.5]\n",
      "427.jpg is a dog\n",
      "[0.5]\n",
      "437.jpg is a dog\n",
      "[0.5]\n",
      "455.jpg is a dog\n",
      "[0.5]\n",
      "463.jpg is a dog\n",
      "[0.5]\n",
      "471.jpg is a dog\n",
      "[0.5]\n",
      "483.jpg is a dog\n",
      "[0.5]\n",
      "487.jpg is a dog\n",
      "[0.5]\n",
      "517.jpg is a dog\n",
      "[0.5]\n",
      "519.jpg is a dog\n",
      "[0.5]\n",
      "533.jpg is a dog\n",
      "[0.5]\n",
      "546.jpg is a dog\n",
      "[0.5]\n",
      "5577.jpg is a dog\n",
      "[0.5]\n",
      "5589.jpg is a dog\n",
      "[0.5]\n",
      "559.jpg is a dog\n",
      "[0.5]\n",
      "5609.jpg is a dog\n",
      "[0.5]\n",
      "5625.jpg is a dog\n",
      "[0.5]\n",
      "5629.jpg is a dog\n",
      "[0.5]\n",
      "5641.jpg is a dog\n",
      "[0.5]\n",
      "5644.jpg is a dog\n",
      "[0.5]\n",
      "5651.jpg is a dog\n",
      "[0.5]\n",
      "5653.jpg is a dog\n",
      "[0.5]\n",
      "5662.jpg is a dog\n",
      "[0.5]\n",
      "5668.jpg is a dog\n",
      "[0.5]\n",
      "5671.jpg is a dog\n",
      "[0.5]\n",
      "5681.jpg is a dog\n",
      "[0.5]\n",
      "5704.jpg is a dog\n",
      "[0.5]\n",
      "5705.jpg is a dog\n",
      "[0.5]\n",
      "5719.jpg is a dog\n",
      "[0.5]\n",
      "5724.jpg is a dog\n",
      "[0.5]\n",
      "5725.jpg is a dog\n",
      "[0.5]\n",
      "5758.jpg is a dog\n",
      "[0.5]\n",
      "576.jpg is a dog\n",
      "[0.5]\n",
      "5762.jpg is a dog\n",
      "[0.5]\n",
      "5766.jpg is a dog\n",
      "[0.5]\n",
      "5768.jpg is a dog\n",
      "[0.5]\n",
      "5769.jpg is a dog\n",
      "[0.5]\n",
      "5773.jpg is a dog\n",
      "[0.5]\n",
      "5774.jpg is a dog\n",
      "[0.5]\n",
      "578.jpg is a dog\n",
      "[0.5]\n",
      "579.jpg is a dog\n",
      "[0.5]\n",
      "5793.jpg is a dog\n",
      "[0.5]\n",
      "5795.jpg is a dog\n",
      "[0.5]\n",
      "580.jpg is a dog\n",
      "[0.5]\n",
      "5805.jpg is a dog\n",
      "[0.5]\n",
      "5830.jpg is a dog\n",
      "[0.5]\n",
      "5847.jpg is a dog\n",
      "[0.5]\n",
      "5851.jpg is a dog\n",
      "[0.5]\n",
      "5854.jpg is a dog\n",
      "[0.5]\n",
      "5866.jpg is a dog\n",
      "[0.5]\n",
      "5874.jpg is a dog\n",
      "[0.5]\n",
      "5877.jpg is a dog\n",
      "[0.5]\n",
      "5883.jpg is a dog\n",
      "[0.5]\n",
      "5890.jpg is a dog\n",
      "[0.5]\n",
      "5898.jpg is a dog\n",
      "[0.5]\n",
      "5907.jpg is a dog\n",
      "[0.5]\n",
      "5911.jpg is a dog\n",
      "[0.5]\n",
      "5935.jpg is a dog\n",
      "[0.5]\n",
      "5946.jpg is a dog\n",
      "[0.5]\n",
      "5957.jpg is a dog\n",
      "[0.5]\n",
      "6034.jpg is a dog\n",
      "[0.5]\n",
      "6041.jpg is a dog\n",
      "[0.5]\n",
      "6054.jpg is a dog\n",
      "[0.5]\n",
      "6063.jpg is a dog\n",
      "[0.5]\n",
      "6074.jpg is a dog\n",
      "[0.5]\n",
      "6076.jpg is a dog\n",
      "[0.5]\n",
      "6080.jpg is a dog\n",
      "[0.5]\n",
      "613.jpg is a dog\n",
      "[0.5]\n",
      "620.jpg is a dog\n",
      "[0.5]\n",
      "626.jpg is a dog\n",
      "[0.5]\n",
      "637.jpg is a dog\n",
      "[0.5]\n",
      "648.jpg is a dog\n",
      "[0.5]\n",
      "666.jpg is a dog\n",
      "[0.5]\n",
      "671.jpg is a dog\n",
      "[0.5]\n",
      "681.jpg is a dog\n",
      "[0.5]\n",
      "693.jpg is a dog\n",
      "[0.5]\n",
      "702.jpg is a dog\n",
      "[0.5]\n",
      "723.jpg is a dog\n",
      "[0.5]\n",
      "726.jpg is a dog\n",
      "[0.5]\n",
      "735.jpg is a dog\n",
      "[0.5]\n",
      "737.jpg is a dog\n",
      "[0.5]\n",
      "747.jpg is a dog\n",
      "[0.5]\n",
      "753.jpg is a dog\n",
      "[0.5]\n",
      "766.jpg is a dog\n",
      "[0.5]\n",
      "783.jpg is a dog\n",
      "[0.5]\n",
      "811.jpg is a dog\n",
      "[0.5]\n",
      "813.jpg is a dog\n",
      "[0.5]\n",
      "814.jpg is a dog\n",
      "[0.5]\n",
      "822.jpg is a dog\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from google.colab import files\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# uploaded=files.upload()\n",
    "\n",
    "# for fn in uploaded.keys():\n",
    "\n",
    "# predicting images\n",
    "#   path='/content/' + fn\n",
    "path = \"C:/Users/josephdavis/Desktop/Tensorflow notebooks/cat-dog-val/\"\n",
    "for fn in os.listdir(path):\n",
    "    img = image.load_img(path + fn, target_size=(150, 150))\n",
    "\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    images = np.vstack([x])\n",
    "\n",
    "    classes = model.predict(images, batch_size=10)\n",
    "\n",
    "    if classes[0] > 0:\n",
    "        print(classes[0])\n",
    "        print(fn + \" is a dog\")\n",
    "\n",
    "    else:\n",
    "        print(fn + \" is a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "emotional-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 5s 227ms/step - loss: 0.1241 - accuracy: 0.9697\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12408275902271271, 0.969672441482544]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "official-laptop",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss [0.26199105381965637, 0.12021912634372711, 0.06868639588356018, 0.04270363226532936, 0.02506813034415245, 0.025179443880915642, 0.017161905765533447, 0.013220086693763733]\n",
      "accuracy [0.8931023478507996, 0.9518916010856628, 0.9743750691413879, 0.9866714477539062, 0.9913835525512695, 0.9928196668624878, 0.9946596026420593, 0.9956917762756348]\n",
      "val_loss [0.10458529740571976, 0.11139388382434845, 0.11649634689092636, 0.12408275902271271, 0.16135023534297943, 0.15425223112106323, 0.17064517736434937, 0.18783418834209442]\n",
      "val_accuracy [0.9660331606864929, 0.9664375185966492, 0.9684593677520752, 0.969672441482544, 0.9672462344169617, 0.9656288027763367, 0.9676506519317627, 0.9664375185966492]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "for value in history.history:\n",
    "    print(value, history.history[value])\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, \"r\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-monte",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

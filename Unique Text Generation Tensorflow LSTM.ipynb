{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-military",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.distribute.one_device_strategy.OneDeviceStrategy at 0x1c580160160>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "primary-mortgage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Even with large batch sizes, there isn't much to speed up with mixed_float due to training inputs batch data. Slows down training on older GPUs\n",
    "\n",
    "# policy = tf.keras.mixed_precision.Policy(\"mixed_float16\")\n",
    "# tf.keras.mixed_precision.experimental.set_policy(policy)\n",
    "tf.config.list_physical_devices('GPU') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cutting-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
      "1122304/1115394 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "path_to_file = tf.keras.utils.get_file(\n",
    "    \"shakespeare.txt\",\n",
    "    \"https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sorted-bearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of test: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, \"rb\").read().decode(encoding=\"utf-8\")\n",
    "print(f\"Length of test: {len(text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "curious-steel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "processed-moore",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(vocab)\n",
    "print(f\"{len(vocab)} unique characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "median-medicaid",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = [\"abcdefg\", \"xyz\"]\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding=\"UTF-8\")\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "naughty-bishop",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "tired-swift",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[41, 42, 43, 44, 45, 46, 47], [64, 65, 66]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "sealed-activation",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "instant-practitioner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "chicken-fundamentals",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "funny-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "literary-princess",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 49, 58, ..., 47, 10,  2], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, \"UTF-8\"))\n",
    "all_ids.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "subjective-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abstract-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "infectious-turner",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text) // (seq_length + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "democratic-australia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length + 1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "    print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cordless-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "impressive-behavior",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "opponent-village",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dirty-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "authentic-hands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Text: b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Input ID's: tf.Tensor(\n",
      "[20 49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45\n",
      "  3 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45\n",
      " 41 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51\n",
      "  8  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12\n",
      "  2 39 55 61], shape=(100,), dtype=int64)\n",
      "Target Text: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "Target ID's: tf.Tensor(\n",
      "[49 58 59 60  3 17 49 60 49 66 45 54 12  2 16 45 46 55 58 45  3 63 45  3\n",
      " 56 58 55 43 45 45 44  3 41 54 65  3 46 61 58 60 48 45 58  8  3 48 45 41\n",
      " 58  3 53 45  3 59 56 45 41 51 10  2  2 15 52 52 12  2 33 56 45 41 51  8\n",
      "  3 59 56 45 41 51 10  2  2 20 49 58 59 60  3 17 49 60 49 66 45 54 12  2\n",
      " 39 55 61  3], shape=(100,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input Text:\", text_from_ids(input_example).numpy())\n",
    "    print(\"Input ID's:\", input_example)\n",
    "    print(\"Target Text:\", text_from_ids(target_example).numpy())\n",
    "    print(\"Target ID's:\", target_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "streaming-viewer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset.shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "serial-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "\n",
    "embedding_dim = 256\n",
    "\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "chubby-collective",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = tf.keras.layers.LSTM(\n",
    "            units=rnn_units, return_sequences=True, return_state=True\n",
    "        )\n",
    "#         self.lstm2 = tf.keras.layers.LSTM(\n",
    "#             units=rnn_units, return_sequences=True, return_state=True\n",
    "#         )\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "\n",
    "        if states is None:\n",
    "            states = self.lstm.get_initial_state(x)\n",
    "        x, states_h, states_c = self.lstm(x, initial_state=states, training=training)\n",
    "        states = [states_h, states_c]\n",
    "        \n",
    "#         x, states_h, states_c = self.lstm2(x, initial_state=states, training=training)\n",
    "#         states = [states_h, states_c]\n",
    "        \n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "rational-festival",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "advisory-scout",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 67) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(\n",
    "        example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "matched-grove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  17152     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  68675     \n",
      "=================================================================\n",
      "Total params: 5,332,803\n",
      "Trainable params: 5,332,803\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "parental-graham",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "declared-distributor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17, 16,  5, 60, 23, 50, 10, 22, 52, 58, 61, 24,  4, 43, 19,  8,  2,\n",
       "        5, 28, 22, 19, 64, 16, 34, 14, 60, 64, 13, 11, 51, 48, 65, 58,  9,\n",
       "       61, 32, 14, 28,  0, 49, 12, 56, 16,  0, 56, 29, 22, 56, 38, 11, 34,\n",
       "       26, 12, 39, 66, 13, 21, 21, 10, 43, 16, 18, 56, 39, 47, 41, 65, 54,\n",
       "       39, 19,  2,  6,  2, 52, 40, 65,  8, 13,  9, 18, 53, 60, 24, 57, 10,\n",
       "        3, 56, 40, 37, 50, 30, 24, 56, 19, 63, 19, 65, 24, 17,  6],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "purple-incentive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'here great Aufidius lies: is he in Antium?\\n\\nCitizen:\\nHe is, and feasts the nobles of the state\\nAt hi'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'CB$tIj.HlruJ!cE,\\n$NHExBT?tx;3khyr-uR?Ni:pBpOHpX3TL:Yz;GG.cBDpYgaynYE\\n&\\nlZy,;-DmtJq. pZWjPJpEwEyJC&'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "voluntary-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "chubby-community",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 67)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss: 4.2056117\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\n",
    "    \"Prediction shape: \",\n",
    "    example_batch_predictions.shape,\n",
    "    \" # (batch_size, sequence_length, vocab_size)\",\n",
    ")\n",
    "print(\"Mean loss:\", mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "addressed-timber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n",
      "67.06161\n"
     ]
    }
   ],
   "source": [
    "# Newly created models shouldn't have weights created that are too certain in it's result without training, the exponent of the mean loss should be close to the number of inputs\n",
    "print(len(vocab))\n",
    "print(tf.exp(mean_loss).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "alleged-segment",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "impossible-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoints directory\n",
    "checkpoint_dir = \"./training_checkpoints\"\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix, save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "beginning-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "# early stopping is cool but since there isn't a validation it doesn't quite work\n",
    "# earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"loss\", mode=\"auto\", patience=3, restore_best_weights=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "hazardous-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "172/172 [==============================] - 20s 97ms/step - loss: 3.2614\n",
      "Epoch 2/30\n",
      "172/172 [==============================] - 18s 97ms/step - loss: 2.1544\n",
      "Epoch 3/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.8518\n",
      "Epoch 4/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.6598\n",
      "Epoch 5/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.5357\n",
      "Epoch 6/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.4472\n",
      "Epoch 7/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.3850\n",
      "Epoch 8/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.3362\n",
      "Epoch 9/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.2856\n",
      "Epoch 10/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.2536\n",
      "Epoch 11/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.2177\n",
      "Epoch 12/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.1840\n",
      "Epoch 13/30\n",
      "172/172 [==============================] - 18s 98ms/step - loss: 1.1470\n",
      "Epoch 14/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 1.1107\n",
      "Epoch 15/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 1.0721\n",
      "Epoch 16/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 1.0326\n",
      "Epoch 17/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.9882\n",
      "Epoch 18/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.9395\n",
      "Epoch 19/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.8917\n",
      "Epoch 20/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.8430\n",
      "Epoch 21/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.7917\n",
      "Epoch 22/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.7413\n",
      "Epoch 23/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.6933 1s -\n",
      "Epoch 24/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.6444\n",
      "Epoch 25/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.5995\n",
      "Epoch 26/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.5589\n",
      "Epoch 27/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.5196\n",
      "Epoch 28/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.4856\n",
      "Epoch 29/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.4535\n",
      "Epoch 30/30\n",
      "172/172 [==============================] - 18s 99ms/step - loss: 0.4215\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "steady-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "    def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        self.chars_from_ids = chars_from_ids\n",
    "        self.ids_from_chars = ids_from_chars\n",
    "\n",
    "        # Create a mask to prevent '' or [UNK] from being Generated\n",
    "        skip_ids = self.ids_from_chars([\"\", \"[UNK]\"])[:, None]\n",
    "        sparse_mask = tf.SparseTensor(\n",
    "            # Put an -inf at each bad index\n",
    "            values=[-float(\"inf\")] * len(skip_ids),\n",
    "            indices=skip_ids,\n",
    "            # Match the shape to the vocabulary\n",
    "            dense_shape=[len(ids_from_chars.get_vocabulary())],\n",
    "        )\n",
    "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "    @tf.function\n",
    "    def generate_one_step(self, inputs, states=None):\n",
    "        # Convert strings to token IDs.\n",
    "        input_chars = tf.strings.unicode_split(inputs, \"UTF-8\")\n",
    "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "        # Run the model.\n",
    "        # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "        predicted_logits, states = self.model(\n",
    "            inputs=input_ids, states=states, return_state=True\n",
    "        )\n",
    "\n",
    "        # Only use the last prediction.\n",
    "        predicted_logits = predicted_logits[:, -1, :]\n",
    "        predicted_logits = predicted_logits / self.temperature\n",
    "        # Apply the prediction mask: prevent \"\" or [UNK] from being generated\n",
    "        predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "        # Sample the output logits to generate token IDs.\n",
    "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "        # Convert from token ids to characters\n",
    "        predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "        # Return the characters and the model state.\n",
    "        return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "commercial-course",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "naughty-missile",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "One of that honour.\n",
      "If thou respect them for their lives like chast.\n",
      "Need thou Romes, Marcius! O, how cares lived, pray,\n",
      "That virestantfur with the desting of my joy\n",
      "But what you do, so well abise; for mill,\n",
      "Who greshall I make a body of a king:\n",
      "itherifference without mildness, present\n",
      "the medely week: if you threw busy-day,\n",
      "What crown his state, for all the lusty gifts\n",
      "have rock'd upon this foot-paped with our compossion.\n",
      "\n",
      "BIANCA:\n",
      "Why, thus's growing she?\n",
      "\n",
      "ROMEO:\n",
      "Thou setter already.\n",
      "\n",
      "First Servingman:\n",
      "Why, here's my daughter: what a trunch, bast are be?\n",
      "I'll still be burnt by not.\n",
      "\n",
      "MIRANDA:\n",
      "Tell me, gentle comm; come to my cause;\n",
      "Where lessours shall go embrace me the matter\n",
      "Than he be pitied much: let 'tis be:\n",
      "I am a goodle's eyes and alter it,\n",
      "Or it conclude their mother in this hour\n",
      "And uped scalue to the Tower. Thou hast no carely\n",
      "Would it were the struct to prove a sweet wife then.\n",
      "How should they, since that I have more care\n",
      "To give the life be slain, whence 'gainst me!\n",
      "He loves the sire deloved so and the\n",
      "Volsces that he is to have him project's\n",
      "Syelves up about their functions: these rettench\n",
      "There is my boing of thine enemies, which\n",
      "You have passed good destineth by calmed\n",
      "That bear in them o' the Capitol; who's the\n",
      "worst, if my tongue being spectation, and go alone\n",
      "The husband and the mind, by death him they do.\n",
      "Here come this whip it born me unpassing your\n",
      "who; the one as I be ceremoniously.\n",
      "\n",
      "CAPULET:\n",
      "Sweet Dukes me an other privious look!\n",
      "\n",
      "SEBASTIAN:\n",
      "I do believe it? you may and hear me? I know not what:\n",
      "The bridegroom issue he me flattered:\n",
      "I, in a simple gallow sirvain, and,\n",
      "though it hose to cross a parlous bilf-blow musty success.\n",
      "I am his gring.\n",
      "\n",
      "Provost:\n",
      "What's his neither comfort? hark your honours star\n",
      "Could ever yet but with a malited with others.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, what ne'er shall prove it profit adversaries?\n",
      "\n",
      "Shervant:\n",
      "One word is coming.\n",
      "\n",
      "MENENIUS:\n",
      "Yet's a love been.\n",
      "\n",
      "PETRUCHIO:\n",
      "I pray you so sweet you will get a month.\n",
      "Lords, saints are s \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.812026262283325\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant([\"ROMEO:\"])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(2000):\n",
    "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "    result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode(\"utf-8\"), \"\\n\\n\" + \"_\" * 80)\n",
    "print(\"\\nRun time:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-greeting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
